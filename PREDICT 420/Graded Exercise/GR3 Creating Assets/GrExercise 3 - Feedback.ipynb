{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRED 420-56 Winter 2016 GrEx3 Comments and Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows are selected bits, examples and comments. Concepts and methods we covered previously are in general omitted.\n",
    "\n",
    "Note that there is more than one way to do most of what this assignment asked you for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data from SSCC and into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered the basic steps in Sync 2.  As I mentioned in our Sync session, creating temporary views wasn't necessary in order to just export csv files.  But, it didn't hurt.\n",
    "\n",
    "In what follows we assume that the customer, mail, and item data have been imported into the pandas DataFrames customerDF, mailDF, and itemDF, respectively.  It's also assumed that pandas has been imported as pd, and numpy as np."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's ok to check for duplicate customer records using values of customer account number, _acctno_, here.  But note that it's possible that the same customer could have two different _acctno's_.   Or, a customer could have the same name and _acctno_ in more than one customer record.  We just don't know for sure.  There are many ways that the same customer can have more than one customer record.  Detecting duplicates has long been an important activity in the direct marketing industry and in other domains. The process has colloquially been called \"de-duping.\" \n",
    "\n",
    "You weren't asked to check for duplicates in the mail data, but it shouldn't have any duplicates for the same customer based on _acctno_.  The reason is that these records summarize for individual customers how many times they have been targeted in a campaign.  So no customer should have more than one record in the mail data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "customerDF['acctno'].duplicated().value_counts()     # any records with the same acctno value?\n",
    "\n",
    "mailDF['acctno'].duplicated().value_counts()         # ditto\n",
    "\n",
    "itemDF.duplicated.value_counts()                     # hm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the item data? There are duplicate records.   What should we make of these?  Based on the field (column) names, is it possible that two or more records can be identical and be valid?   It's hard to tell based on the column names.  If you look at the values of the _qty_ column you'll see that they range from 1 to a very large integer.  The same is true for _orderno_.  As many as 93 records have the same value of _orderno_.   This might be because multiple items ordered in varying quantities comprise the records in the items data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that item or mail records are associated with a customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to go about doing this.  One way is to use the .asin() method, e.g.:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mailDF.acctno.isin(customerDF.acctno).value_counts()    #simple check to see if all acctno values in mail are in customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you found _acctno_ values in the mail or item data that weren't in the customer data, you could use the same idiom to select just the mail or item records associated with an _acctno_ in the customer data, e,g."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "okMailDF=mailDF[mailDf.acctno.isin(customerDF.acctno)]   # a new df with mail records that have acctno's in customer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a SQLite DB and Write Tables To It"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use SQLAlchemy, here.  You could use the Python sqlite3 module, pandas.io.sql, or other means, instead.  We'll write mailDF, itemDF, and customerDF to a SQLite3 db as tables in the very simplest manner possible.  We'll assume here that what's in these DF's has been adequately \"scrubbed.\" (Not always a safe assumption with a real application, of course.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine=create_engine('sqlite:///myxyzdata.db')   # this will be in our current working directory\n",
    "conn=engine.connection()                         # define an explicit connection in the pool engine provides\n",
    "itemDF.to_sql('item',conn)                       # write itemDF into myxyzdata.db as table\n",
    "mailDF.to_sql('mail',conn)                       # mailDF as a table\n",
    "customerDF.to_sql('customer',conn)               # and, customerDF as a table\n",
    "\n",
    "# The three DFs should now be tables in myxyzdata.db.  We can see if there are tables in there:\n",
    "\n",
    "from sqlalchemy import inspect\n",
    "insp=inspect(conn)\n",
    "insp.get_table_names()     # this should return the table names item, mail, customer\n",
    "conn.close()               # this closes the explicit connection to the db that we used here\n",
    "                           # note that engine will always be defined throughout the current Python session\n",
    "                           \n",
    "# Count of records in the SQLite3 DB, for example, for the mail table:\n",
    "\n",
    "conn=engine.connect()      # this assumes the connection was closed, as above.  Still the same session.\n",
    "\n",
    "pd.read_sql('mail',conn).shape     # this does require reading the table into RAM.  a SQL statement doesn't:\n",
    "\n",
    "pd.read_sql_query(\"SELECT COUNT(*) FROM item\",conn)    # This will return a DF\n",
    "\n",
    "conn.close()               # A good thing to remember to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLAlchemy is a very comprehensive resource with many useful features and methods. Documentation for it is available at http://www.sqlalchemy.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CSV File for a Direct Marketing Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do this using what we saved into our SQLiteDB, `myxyzdata.db`. Or, we could use the data we loaded into Python after having checked it for duplicates and for \"orphan\" item and mail records. We'll do the latter, here.  Note the variable names, here.  Did you have the same names?  Also, need to check for duplicates in the final result?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create a new DF using selected columns in customerDF\n",
    "\n",
    "custsDF=pd.DataFrame(customerDF,columns=['acctno','ytd_transactions_2009','ytd_sales_2009','zmobav','zhomeent'])\n",
    "\n",
    "# The 0/1 mailing indicaters in mailDF are type integer, so we can sum across them\n",
    "\n",
    "mailDF['noMailings']=mailDF.iloc[:,1:].sum(axis=1)     # sum() is applied across all cols except acctno, max=16\n",
    "                                                       # Instead of specifying a range of cols, you could use \n",
    "                                                       # the numeric_only option in sum()\n",
    "\n",
    "mailDF=mailDF[[\"accntno','noMailings']]     # keep just acctno, noMailings (Beware of copy by reference, always!)\n",
    "    \n",
    "targetCustsDf=pd.merge(custsDF,mailDF,how='right')     # merge on acctno, retain just accts in mailDF\n",
    "\n",
    "targetCustsDF=targetCustsDF[targetCustsDF.noMailings>=5]    # Select customers qualified for targeting\n",
    "\n",
    "# Next, create the o/1 versions of zmobav and zhomeent.  Are there any NaNs?  \n",
    "# One way: we can .apply() a baby fcn:\n",
    "\n",
    "def YtoOne(x):\n",
    "    if x==\"Y\":\n",
    "        return 1\n",
    "    else: return 0\n",
    "    \n",
    "targetCustsDF['zmobav01']=targetCustsDF.zmobav.apply(YtoOne)     # applying a fcn to each element in a Series, here.\n",
    " \n",
    "targetCustsDF['zhomeent01']=targetCustsDF.zhomeent.apply(YtoOne) # could .applymap() be used, here?\n",
    " \n",
    "# Now, check by crosstabulating each of the two z variables and their 0/1 versions. For zmobav and zmobav01, for example:\n",
    "       \n",
    "pd.crosstab(targetCustsDF.zmobav.fillna('missing'),targetCustsDF.zmobav01.fillna('missing'),dropna=True,margins=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fillna() methods are kind of a work-around for a pd.crosstab ideosyncracy.  We could have replaced NaN's with a little code in the YtoOne() function, instead, but that would have changed what's in the new variables. We don't want that because the targetCustsDF is to be written out to a CSV file and will be pickled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the assignment involves writing a CSV file, and pickling, which are things you already know how to do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
